<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    
    <title>Spark学习之路（一） | DylanWen&#39;s  Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content="Spark" />
    
    <meta name="description" content="研究生考试终于告一段落，但学习的脚步仍然不能停止。由于是往届生，在学校也没有住宿，校外住宿也不大方便，于是回来学习。为了尽快进入学习状态以及跟上实验室其他同学，张旭教授要求每周汇报一次学习进度，准备在自己的博客进行书写。 要求学习的内容及工具如下：  语言方面：   Java Python Scala    平台方面：   Spark Hive Processing      现阶段已经着手在电脑">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习之路（一）">
<meta property="og:url" content="http://wenshunjie.top/posts/programming/2019-04-04-Spark学习之路（一）.html">
<meta property="og:site_name" content="DylanWen&#39;s  Blog">
<meta property="og:description" content="研究生考试终于告一段落，但学习的脚步仍然不能停止。由于是往届生，在学校也没有住宿，校外住宿也不大方便，于是回来学习。为了尽快进入学习状态以及跟上实验室其他同学，张旭教授要求每周汇报一次学习进度，准备在自己的博客进行书写。 要求学习的内容及工具如下：  语言方面：   Java Python Scala    平台方面：   Spark Hive Processing      现阶段已经着手在电脑">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark-logo-trademark.png">
<meta property="og:updated_time" content="2019-04-08T14:19:02.419Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark学习之路（一）">
<meta name="twitter:description" content="研究生考试终于告一段落，但学习的脚步仍然不能停止。由于是往届生，在学校也没有住宿，校外住宿也不大方便，于是回来学习。为了尽快进入学习状态以及跟上实验室其他同学，张旭教授要求每周汇报一次学习进度，准备在自己的博客进行书写。 要求学习的内容及工具如下：  语言方面：   Java Python Scala    平台方面：   Spark Hive Processing      现阶段已经着手在电脑">
<meta name="twitter:image" content="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark-logo-trademark.png">
    

    

    
        <link rel="icon" href="/cake.ico" />
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.0.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f8403e7c46b36682c7b150f2f5211325";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


</head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">生活本可更精彩</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">主页</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/life">过日子</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/programming">滚键盘</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/english">念洋文</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about">关于</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form" method="GET" action="https://www.baidu.com/s?">
    <input name="wd" type="text" class="search-form-input" placeholder="搜索" />
</form>
<script>
(function ($) {
    $('.search-form').on('submit', function (e) {
        var keyword = $('.search-form-input[name="wd"]').val();
        window.location = 'https://www.baidu.com/s?wd=site:wenshunjie.top ' + keyword;
        return false;
    });
})(jQuery);
</script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>
        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/programming/">滚键盘</a>
    </h1>
</div>
                        <div class="main-body-content">
                            <article id="post-Spark学习之路（一）" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Spark学习之路（一）
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/posts/programming/2019-04-04-Spark学习之路（一）.html" class="article-date">
            <time datetime="2019-04-04T02:45:49.000Z" itemprop="datePublished">2019-04-04</time>
        </a>
    </div>

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/Spark/">Spark</a>
    </div>

                阅读数量:<span id="/posts/programming/2019-04-04-Spark学习之路（一）.html" class="leancloud_visitors" data-flag-title="Spark学习之路（一）"></span>次
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <p>研究生考试终于告一段落，但学习的脚步仍然不能停止。由于是往届生，在学校也没有住宿，校外住宿也不大方便，于是回来学习。为了尽快进入学习状态以及跟上实验室其他同学，张旭教授要求每周汇报一次学习进度，准备在自己的博客进行书写。</p>
<p>要求学习的内容及工具如下：</p>
<ul>
<li><p>语言方面：</p>
<blockquote>
<ul>
<li>Java</li>
<li>Python</li>
<li>Scala</li>
</ul>
</blockquote>
</li>
<li><p>平台方面：</p>
<blockquote>
<ul>
<li>Spark</li>
<li>Hive</li>
<li>Processing</li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark-logo-trademark.png" alt="Spark"></p>
<p>现阶段已经着手在电脑上使用<strong>VMware Workstation Pro 14</strong>来搭建环境，具体配置如下：</p>
<ul>
<li><p>硬件：</p>
<blockquote>
<ul>
<li>RAM:2GB</li>
<li>ROM:20GB</li>
<li>CORE:4</li>
<li>主机/从机:1master,1slave</li>
</ul>
</blockquote>
</li>
<li><p>软件：</p>
<blockquote>
<ul>
<li>Ubuntu:CentOS-7-x86_64-DVD-1810</li>
<li>Hadoop：3.2.0</li>
<li>Spark：2.2.3</li>
<li>Scala：2.11.12</li>
<li>Python：3.7.3</li>
<li>Anaconda：3.0</li>
<li>Jdk：1.8.0_201</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="为什么使用Spark来进行数据处理"><a href="#为什么使用Spark来进行数据处理" class="headerlink" title="为什么使用Spark来进行数据处理"></a>为什么使用Spark来进行数据处理</h2><h3 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h3><p><strong>Apache Spark</strong>是开放源码的集群运算框架，由加州大学伯克利分校的AMPLab开发。Spark是一个弹性的运算框架，适合进行Spark Streaming数据流处理、Spark SQL互动分析、MLlib机器学习等应用，因此Spark可作为一个用途广泛的大数据运算平台。Spark允许用户将数据加载到cluster集群的内存中存储，并多次重复运算，非常适合用于机器学习的算法。</p>
<p>Spark的核心是<strong>RDD(Resilient Distributed Dataset)弹性分布式数据集</strong>，是由AMPLab实验室所提出的的概念，属于一种分布式的内容。<br>Apache Spark是一种快速的集群计算技术，专为快速计算而设计。它基于Hadoop MapReduce，它扩展了MapReduce模型，以有效地将其用于更多类型的计算，包括交互式查询和流处理。 Spark的主要特性是它的内存中集群计算，提高了应用程序的处理速度。</p>
<p>Spark旨在涵盖各种工作负载，如批处理应用程序，迭代算法，交互式查询和流式处理。除了在相应系统中支持所有这些工作负载之外，它还减少了维护单独工具的管理负担。</p>
<h3 id="Spark和Hadoop的关系"><a href="#Spark和Hadoop的关系" class="headerlink" title="Spark和Hadoop的关系"></a>Spark和Hadoop的关系</h3><p>行业广泛使用Hadoop来分析他们的数据集。原因是Hadoop框架基于一个简单的编程模型（MapReduce），它支持可扩展，灵活，容错和成本有效的计算解决方案。这里，主要关注的是在处理大型数据集时在查询之间的等待时间和运行程序的等待时间方面保持速度。<br>Spark由Apache Software Foundation引入，用于加速Hadoop计算软件过程。<br>对于一个普遍的信念，Spark不是Hadoop的修改版本，并不是真的依赖于Hadoop，因为它有自己的集群管理。 Hadoop只是实现Spark的方法之一。<br>Spark以两种方式使用Hadoop，一个是存储，另一个是处理。由于Spark具有自己的集群管理计算，因此它仅使用Hadoop进行存储。<br><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/Hadoop%E5%92%8CSpark.jpg" alt="Hadoop和Spark"></p>
<h3 id="park的优势"><a href="#park的优势" class="headerlink" title="park的优势"></a>park的优势</h3><p>Apache Spark具有以下优势：</p>
<ol>
<li><p>速度</p>
<p>Spark有助于在Hadoop集群中运行应用程序，在内存中速度提高100倍，在磁盘上运行时提高10倍。这可以通过减少对磁盘的读/写操作的数量来实现。它将中间处理数据存储在存储器中。</p>
</li>
<li><p>支持多种语言</p>
<p>Spark在Java，Scala或Python中提供了内置的API。因此，您可以使用不同的语言编写应用程序。 Spark提供了80个高级操作员进行交互式查询。</p>
</li>
<li><p>高级分析</p>
<p>Spark不仅支持“Map”和“reduce”。它还支持SQL查询，流数据，机器学习（ML）和图算法。</p>
</li>
</ol>
<h3 id="Spark基于Hadoop"><a href="#Spark基于Hadoop" class="headerlink" title="Spark基于Hadoop"></a>Spark基于Hadoop</h3><p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark_built_on_hadoop.jpg" alt="Hadoop组件构建Spark的三种方式"></p>
<p>Spark部署有三种方式，如下所述:</p>
<ol>
<li><p>Standalone- Spark独立部署意味着Spark占据HDFS（Hadoop分布式文件系统）顶部的位置，并明确为HDFS分配空间。 这里，Spark和MapReduce将并行运行以覆盖集群上的所有spark作业。</p>
</li>
<li><p>Hadoop Yarn- Hadoop Yarn部署意味着，spark只需运行在Yarn上，无需任何预安装或根访问。 它有助于将Spark集成到Hadoop生态系统或Hadoop堆栈中。 它允许其他组件在堆栈顶部运行。</p>
</li>
<li><p>Spark in MapReduce (SIMR) - MapReduce中的Spark用于在独立部署之外启动spark job。 使用SIMR，用户可以启动Spark并使用其shell而无需任何管理访问。</p>
</li>
</ol>
<h3 id="Spark组件"><a href="#Spark组件" class="headerlink" title="Spark组件"></a>Spark组件</h3><p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/components_of_spark.jpg" alt="Spark组成部分"></p>
<ol>
<li><p>Apache Spark Core</p>
<p>Spark Core是spark平台的基础通用执行引擎，所有其他功能都是基于。它在外部存储系统中提供内存计算和引用数据集。</p>
</li>
<li><p>Spark SQL</p>
<p>Spark SQL是Spark Core之上的一个组件，它引入了一个称为SchemaRDD的新数据抽象，它为结构化和半结构化数据提供支持。</p>
</li>
<li><p>Spark Streaming</p>
<p>Spark Streaming利用Spark Core的快速调度功能来执行流式分析。它以小批量获取数据，并对这些小批量的数据执行RDD（弹性分布式数据集）转换。</p>
</li>
<li><p>MLlib (Machine Learning Library)</p>
<p>MLlib是Spark之上的分布式机器学习框架，因为基于分布式内存的Spark架构。根据基准，它是由MLlib开发人员针对交替最小二乘法（ALS）实现完成的。 Spark MLlib是基于Hadoop磁盘的Apache Mahout版本的9倍（在Mahout获得了Spark接口之前）。</p>
</li>
<li><p>GraphX<br>GraphX是Spark上的一个分布式图形处理框架。它提供了一个用于表达图形计算的API，可以通过使用Pregel抽象API为用户定义的图形建模。它还为此抽象提供了一个优化的运行时。</p>
</li>
</ol>
<h3 id="Python-Spark机器学习"><a href="#Python-Spark机器学习" class="headerlink" title="Python Spark机器学习"></a>Python Spark机器学习</h3><p>Python机器学习膜卷主要是Pandas、Scikit-learn，但是在大数据时代有大量的数据，必须具有分布式存储以及分布式计算才能够处理。有了Spark之后，使用Python开发Spark应用程序，可以使用HDFS分布式存储大量数据。还可以使用在多台计算机所建立的集群（例如：Spark stand alone、Hadoop YARN、Mesos）上来执行分布式计算。再加上Spark特有的内存运算，让执行速度大幅提升。</p>
<p>Spark机器学习主要有两个API：</p>
<ol>
<li><p>Spark MLlib:RDD-based 机器学习API：<br>Spark在一开始就提供了以RDD为基础的机器学习模块，优点是可以发挥in-memory与分布式运算，大幅提升需要迭代的机器学习模块的执行效率，功能强大，能完成Spark所有功能。</p>
</li>
<li><p>Spark ML pipeline:DataFrames-based 机器学习API:<br>DataFrame与Spark Pipeline机器学习API的设计由来如下：</p>
<blockquote>
<p>DataFrame : Spark受Pandas程序包启发所设计的数据处理架构。<br>Spark Pipeline : Spark受Scikit-learn程序包启发所设计的机器学习架构。</p>
</blockquote>
</li>
</ol>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BD%AF%E4%BB%B6%E5%8C%85.jpg" alt="常用数据分析软件包"></p>
<h2 id="Linux虚拟机与Hadoop分布式平台搭建"><a href="#Linux虚拟机与Hadoop分布式平台搭建" class="headerlink" title="Linux虚拟机与Hadoop分布式平台搭建"></a>Linux虚拟机与Hadoop分布式平台搭建</h2><p>在毕设的时候采用的也是VMvare运行CentOS7的Linux虚拟机，这次采用Ubuntu的系统，开始觉得应该大同小异，并且以前配置过环境，会比较轻车熟路，但是实际上到Pyspark正常运行也花了两天时间，期间踩了无数的坑，为了以后更好的配置环境，现记录如下，并且汇总自己的踩坑记录。</p>
<h3 id="安装Ubuntu"><a href="#安装Ubuntu" class="headerlink" title="安装Ubuntu"></a>安装Ubuntu</h3><p>系统安装过程不难，需要注意到是Ubuntu输入法的设置以及设置共享剪贴板。</p>
<h4 id="输入法设置"><a href="#输入法设置" class="headerlink" title="输入法设置"></a>输入法设置</h4><p>由于安装的时候选择语言是汉语，但是大部分情况下输入命令和程序都是英文，必须自行切换输入法，这样极其不方便，所以最好把默认输入法改成英文并且把输入法切换方式改成我们Windows常用的方式。</p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/language.jpg" alt="输入法设置"></p>
<p>在设置&gt;设备&gt;键盘中进行切换方式的设置。</p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/language_shift.jpg" alt="输入法切换"></p>
<h4 id="共享剪贴板设置"><a href="#共享剪贴板设置" class="headerlink" title="共享剪贴板设置"></a>共享剪贴板设置</h4><p>在VMvare软件中，自带有VMTools工具，可以将Windows和虚拟机共享剪贴板，挂载VMTools进行解压安装即可。</p>
<p>至于选择最佳下载服务器，这个我觉得不太必要设置。</p>
<h3 id="Hadoop分布式系统配置"><a href="#Hadoop分布式系统配置" class="headerlink" title="Hadoop分布式系统配置"></a>Hadoop分布式系统配置</h3><h4 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h4><p>Hadoop是使用Java开发的，所以必须先安装JDK。由于在安装Ubuntu的时候已经选择了安装开发环境，所以已经自动安装好了较高版本的Java，但是这也是影响我后面无法启动ResorceManager进程的一个原因（详见爬坑记录）。</p>
<p>所以我最终是采用Jdk:1.8.0_201：</p>
<p><strong>查询Java版本</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Java -version</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/java-version.jpg" alt="Java版本"></p>
<p><strong>查询Java安装路径</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update-alternatives --display Java</span><br></pre></td></tr></table></figure>
<h4 id="SSH免密登陆"><a href="#SSH免密登陆" class="headerlink" title="SSH免密登陆"></a>SSH免密登陆</h4><p>Hadoop是由多台服务器组成，再启动Hadoop系统时，NameNode必须与DataNode连接并且管理这些节点。系统会要求用户输入密码，为了系统顺利运行而不手动输入密码，通常采用SSH设置成无密码登陆。在安装系统的时候好像也已经存在了SSH和rsync，以防万一补充安装方法。</p>
<p><strong>安装SSH</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure>
<p><strong>安装rsync</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install rsync</span><br></pre></td></tr></table></figure>
<p><strong>主节点免密登陆</strong></p>
<p>在主节点的终端输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>然后一路回车默认安装位置，如果已经存在就覆盖。</p>
<p><strong>将key放到许可证文件并修改权限</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 600 authorized_keys</span><br><span class="line">chmod 700 .ssh</span><br></pre></td></tr></table></figure>
<p><strong>将authorized_keys发送到从节点data1</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp authorized_keys hduser@data1:~/.ssh</span><br></pre></td></tr></table></figure></p>
<p>同样地也修改从节点处的权限。</p>
<p><strong>SSH访问</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh data1</span><br></pre></td></tr></table></figure>
<h4 id="下载安装Hadoop"><a href="#下载安装Hadoop" class="headerlink" title="下载安装Hadoop"></a>下载安装Hadoop</h4><p>两种方式我觉得都比较方便，一种是采用wget命令，进行远程下载，另一种是在Windows系统下载之后通过VMTools的共享文件夹的方式传输的Linux下进行解压安装。</p>
<p><strong>wget下载</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz</span><br></pre></td></tr></table></figure>
<p><strong>解压缩Hadoop</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf hadoop-3.2.0.tar.gz</span><br></pre></td></tr></table></figure>
<p><strong>移动Hadoop到/usr/local</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv hadoop-3.2.0 /usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure>
<h4 id="设置Hadoop环境变量"><a href="#设置Hadoop环境变量" class="headerlink" title="设置Hadoop环境变量"></a>设置Hadoop环境变量</h4><p>每个组件配置环境变量是最关键的一环，并且版本不同的Hadoop在环境变量的配置上也有细微的差别，由于我采用了最新的3.2.0的版本，踩了许多坑，详见爬坑记录，现附我已经配置成功的主、从机代码。</p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/Hadoop-version.jpg" alt="Hadoop版本"></p>
<h5 id="主机"><a href="#主机" class="headerlink" title="主机"></a>主机</h5><p><strong>编辑~/.bashrc</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>该文件是系统整体环境变量的核心，想要直接通过hadoop、java、scala等命令访问，就必须配置好这个文件，在文件末尾加入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin </span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_HOME</span> </span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native </span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"-Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib"</span> </span><br><span class="line"><span class="built_in">export</span> JAVA_LIBRARY_PATH=<span class="variable">$HADOOP_HOME</span>/lib/native:<span class="variable">$JAVA_LIBRARY_PATH</span></span><br><span class="line"><span class="comment"># hadoop</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib/tools.jar</span><br><span class="line"><span class="comment"># scala</span></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;SCALA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="comment"># spark</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="comment"># python</span></span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$ANACONDA_PATH</span>/bin/python</span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=<span class="variable">$ANACONDA_PATH</span>/bin/python</span><br><span class="line"><span class="comment"># anaconda</span></span><br><span class="line"><span class="built_in">export</span> PATH=/home/hduser/anaconda3/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> ANACONDA_PATH=/home/hduser/anaconda3</span><br><span class="line"><span class="built_in">export</span> PYSPARK_DRIVER_PYTHON=<span class="variable">$ANACONDA_PATH</span>/bin/ipython</span><br></pre></td></tr></table></figure>
<p>配置完成之后，从系统注销再登陆系统，设置可以立即生效，或者使用source是其生效，输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><strong>hostname主机名</strong></p>
<p>在主机的hostname中输入master，将其作为namenode。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/hostname</span><br></pre></td></tr></table></figure>
<p><strong>Hadoop-env.sh</strong></p>
<p>打开文件的操作不再赘述，直接附上我的配置文件，在最后加上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201</span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"-Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib"</span> </span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=hduser</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=hduser</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=hduser</span><br></pre></td></tr></table></figure>
<p><strong>core-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上本地的hadoop临时文件夹<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>YARN-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>mapred-site.xml</strong></p>
<p>首先复制模板文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /usr/<span class="built_in">local</span>/hadoop/etc/hadoop/mapred-site.xml.template  /usr/<span class="built_in">local</span>/hadoop/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>vix.mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hadoop_data/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>创建并格式化HDFS目录</strong></p>
<p>创建文namenode数据存储目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /usr/<span class="built_in">local</span>/hadoop/hadoop_data/hdfs/namenode</span><br></pre></td></tr></table></figure></p>
<p>设置Hadoop目录的所有者为hduser：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown hduser:hduser -R /usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure>
<p>格式化HDFS：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>
<h5 id="从机"><a href="#从机" class="headerlink" title="从机"></a>从机</h5><p><strong>hostname主机名</strong></p>
<p>在主机的hostname中输入data1，将其作为datanode。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/hostname</span><br></pre></td></tr></table></figure>
<p><strong>core-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>YARN-site.xml</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8025<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8050<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>mapred-site.xml</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:54311<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>hdfs-site.xml</strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 上面这个数值一般不超过从机数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hadoop_data/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h4 id="主机克隆到从机时需要注意"><a href="#主机克隆到从机时需要注意" class="headerlink" title="主机克隆到从机时需要注意"></a>主机克隆到从机时需要注意</h4><p><strong>配置网卡interfaces</strong></p>
<p>在VMVare的虚拟机设置中添加一个网卡，且设置为仅连主机模式。此时一张网卡NAT模式连接互联网，另一张网卡用于主从及互相联通。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/network/interfaces</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/interfaces.jpg" alt="data1-interfaces"></p>
<p><strong>确认网络配置</strong></p>
<p>输入命令确认网络设置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/ifconfig.jpg" alt="网络配置"></p>
<p>如果修改interfaces的方法无法成功修改网络配置，可以在图形界面的网络连接里自行手动修改。</p>
<p><strong>主机SSH登陆从机创建HDFS相关目录</strong></p>
<p>删除HDFS所有目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /usr/<span class="built_in">local</span>/hadoop/hadoop_data/hdfs</span><br></pre></td></tr></table></figure>
<p>创建DataNode存储目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/<span class="built_in">local</span>/hadoop/hadoop_data/hdfs/datenode</span><br></pre></td></tr></table></figure>
<p>目录的所有者改为hduser</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R hduser:hduser /usr/<span class="built_in">local</span>/hadoop</span><br></pre></td></tr></table></figure>
<p>####启动Hadoop集群</p>
<p><strong>master主机启动集群</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>
<p><strong>输入jps查看正在运行的进程</strong></p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/master-jps.jpg" alt="master-jps"></p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/data1-jps.jpg" alt="data1-jps"></p>
<p><strong>NameNode Web界面</strong></p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/HDFS%20Web.jpg" alt="HDFS Web"></p>
<h4 id="Hadoop总结"><a href="#Hadoop总结" class="headerlink" title="Hadoop总结"></a>Hadoop总结</h4><p>在Hadoop集群这块仍存在疑问没有解决，曾经毕设的时候使用老版本的Hadoop没有发生过，现在存在了，暂时还未彻底找到完美解决办法：</p>
<ol>
<li><p>master主机始终存在DataNode进程</p>
</li>
<li><p>master主机使用start-all.sh的时候，data1从机DataNode等进程无法联动自动启动，不过手动启动不影响使用，但是在8088的Hadoop ResourceManager Web页面中显示的节点是master的而非data1的。</p>
</li>
</ol>
<h3 id="HDFS命令实现"><a href="#HDFS命令实现" class="headerlink" title="HDFS命令实现"></a>HDFS命令实现</h3><p>在启动了集群之后，尝试着使用HDFS命令实现一些基本功能。</p>
<h4 id="常用HDFS命令"><a href="#常用HDFS命令" class="headerlink" title="常用HDFS命令"></a>常用HDFS命令</h4><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>hadoop fs -mkdir</td>
<td>创建HDFS目录</td>
</tr>
<tr>
<td>hadoop fs -ls</td>
<td>列出HDFS目录</td>
</tr>
<tr>
<td>hadoop fs -copyFromLocal</td>
<td>使用-copyFromLocal复制本地文件到HDFS</td>
</tr>
<tr>
<td>hadoop fs -put</td>
<td>使用-put复制本地文件到HDFS</td>
</tr>
<tr>
<td>hadoop fs -cat</td>
<td>列出HDFS目录下的内容</td>
</tr>
<tr>
<td>hadoop fs -copyToLocal</td>
<td>使用-copyToLocal将HDFS复制到本地</td>
</tr>
<tr>
<td>hadoop fs -get</td>
<td>使用-get将HDFS文件复制到本地</td>
</tr>
<tr>
<td>hadoop fs -cp</td>
<td>复制HDFS文件</td>
</tr>
<tr>
<td>hadoop fs -rm</td>
<td>删除HDFS文件</td>
</tr>
</tbody>
</table>
<h3 id="MapReduce命令实现"><a href="#MapReduce命令实现" class="headerlink" title="MapReduce命令实现"></a>MapReduce命令实现</h3><p>MapReduce是一种程序开发模式，可以使用大量服务器并行处理。Map就是分配工作，Reduce就是将工作结果汇总整理。</p>
<p>首先使用Map将待处理的数据分割成很多小份数据，由每台服务器分别运行。</p>
<p>再通过Raeduce程序进行数据合并，最后汇总整理结果。</p>
<h4 id="试运行WordCount-java"><a href="#试运行WordCount-java" class="headerlink" title="试运行WordCount.java"></a>试运行WordCount.java</h4><h5 id="编辑WordCount-java"><a href="#编辑WordCount-java" class="headerlink" title="编辑WordCount.java"></a>编辑WordCount.java</h5><p><strong>创建wordcount测试目录</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/wordcount/input</span><br></pre></td></tr></table></figure>
<p><strong>切换至wordcount测试目录</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/wordcount</span><br></pre></td></tr></table></figure>
<p><strong>复制WordCount.java程序代码</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit WordCount.java</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span></span></span><br><span class="line"><span class="class">       <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context</span></span></span><br><span class="line"><span class="function"><span class="params">                    )</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span></span></span><br><span class="line"><span class="class">       <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">                       Context context</span></span></span><br><span class="line"><span class="function"><span class="params">                       )</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="编译WordCount-java"><a href="#编译WordCount-java" class="headerlink" title="编译WordCount.java"></a>编译WordCount.java</h5><p><strong>编译WordCount程序</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop com.sun.tools.javac.Main WordCount.java</span><br></pre></td></tr></table></figure>
<p><strong>把WordCount类打包成wc.jar</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jar cf wc.jar WordCount*.class</span><br></pre></td></tr></table></figure>
<h5 id="创建测试文本文件"><a href="#创建测试文本文件" class="headerlink" title="创建测试文本文件"></a>创建测试文本文件</h5><p><strong>复制文本</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/<span class="built_in">local</span>/hadoop/LICENSE.txt ~/wordcount/input</span><br></pre></td></tr></table></figure>
<p><strong>上传测试文件到HDFS目录</strong></p>
<p>在HDFS创建目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/hduser/wordcount/input</span><br></pre></td></tr></table></figure>
<p>切换到数据文件目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/wordcount/input</span><br></pre></td></tr></table></figure>
<p>上传文本文件到HDFS<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal LICENSE.txt /user/hduser/wordcount/input</span><br></pre></td></tr></table></figure></p>
<p>列出HDFS文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /user/hduser/wordcount/input</span><br></pre></td></tr></table></figure>
<h5 id="运行WordCount-java"><a href="#运行WordCount-java" class="headerlink" title="运行WordCount.java"></a>运行WordCount.java</h5><p>切换目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/wordcount</span><br></pre></td></tr></table></figure>
<p>运行WordCount程序</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar wc.jar WordCount /user/hduser/wordcount/input/LICENSE.txt /user/hduser/wordcount/output</span><br></pre></td></tr></table></figure>
<h5 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /user/hduser/wordcount/output/part-r-00000|more</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/wordcount.jpg" alt="wordcount结果"></p>
<h4 id="MapReduce总结"><a href="#MapReduce总结" class="headerlink" title="MapReduce总结"></a>MapReduce总结</h4><p>基本把MapReduce进行了一遍，但是它同时也存在一些缺点：</p>
<ol>
<li>程序设计模式不容易使用，而且Hadoop的MapReduce API太过低级，很难提高开发者效率</li>
<li>有运行效率的问题，MapReduce需要将中间产生的数据保存的硬盘中，因此会有读写数据延迟的问题</li>
<li>不支持实时处理，他原始的设计就是以批处理为主</li>
</ol>
<h3 id="Python-Spar安装"><a href="#Python-Spar安装" class="headerlink" title="Python Spar安装"></a>Python Spar安装</h3><h4 id="Spark-Cluster模式"><a href="#Spark-Cluster模式" class="headerlink" title="Spark Cluster模式"></a>Spark Cluster模式</h4><p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/cluster-overview.png" alt="Spark Cluser模式架构图"></p>
<blockquote>
<ul>
<li>DriverProgram就是程序员所涉及的Spark程序，在Spark程序中必须定义SparkContext，是开发Spark应用的入口。</li>
<li>SparkContext通过Cluster管理整个集群，集群中包含多个WorkerNode，在每个WorkNode中都有Executor负责执行任务。</li>
</ul>
</blockquote>
<p>SparkContext通过Cluster Manager管理整个集群Cluster的好处是：所设计的Spark程序可以在不同的Cluster模式下运行。</p>
<p><strong>Cluster Manager可以在下列模式运行</strong></p>
<blockquote>
<ul>
<li>本地运行(Local Machine)——只需在程序中import Spark的链接库就可以实现。在本地运行时，对于只安装在一台计算机上最方便，适合刚入门时学习、测试使用</li>
<li>Spark Standalone Cluster——由Spark提供的Cluster管理模式，若没有架设Hadoop Multi Node Cluster，则可以架设Spark Standalone Cluster，实现同台计算机并行计算，在这个模式下仍然可以直接存取Local Disk或者HDFS</li>
<li>Hadoop YARN——Hadoop 2.x or later 新架构中更高效率的资源管理核心。Spark可以在YARN上运行，让YARN帮助它进行多台机器的资源管理。</li>
<li>云端运行——针对更大规模的计算工作，本地机器或者自建集群的计算能力恐怕难以满足，此时可以将Spark程序在云平台上运行，好处是不用自己架设的费用，用多少付多少，可以随时扩容。</li>
</ul>
</blockquote>
<h4 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h4><p>安装的方式不再赘述，基本上都是大同小异，环境配置整体的也在前文有提。</p>
<p><strong>查看Scala版本</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -version</span><br></pre></td></tr></table></figure></p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/Scala-version.jpg" alt="Scala 2.11.12"></p>
<p><strong>启动Scala</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/Scala-run.jpg" alt="Scala运行"></p>
<h4 id="Spark安装"><a href="#Spark安装" class="headerlink" title="Spark安装"></a>Spark安装</h4><p>pyspark的优点是具有交互的好处，输入命令立刻就可以看到响应。</p>
<p>安装的方式不再赘述，在安装之后尝试启动。</p>
<p><strong>启动pyspark</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark.jpg" alt="启动pyspark"></p>
<p><strong>退出pyspark</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>exit()</span><br></pre></td></tr></table></figure>
<h4 id="本地运行Spark"><a href="#本地运行Spark" class="headerlink" title="本地运行Spark"></a>本地运行Spark</h4><p><strong>pyspark本地运行</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master <span class="built_in">local</span>[*]</span><br></pre></td></tr></table></figure>
<p>local[N]代表在本地运行，使用N个线程，也就是说可以同时执行N个程序。虽然是本地运行，但现在CPU大多数多核心，所以用多线程仍然会加速执行。local[*]会尽量使用我们机器上的CPU核心，我们也可以指定使用的线程数。</p>
<p><strong>查看运行模式</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.master</span><br></pre></td></tr></table></figure>
<p><strong>读取本地文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>textFile=sc.textFile(<span class="string">"file:/usr/local/spark/README.md"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>显示项数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>textFile.count()</span><br></pre></td></tr></table></figure>
<p><strong>读取HDFS文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>textFile=sc.textFile(<span class="string">"hdfs://master:9000/user/hduser/wordcount/input/LICENSE.txt"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Hadoop-YAEN运行pyspark"><a href="#Hadoop-YAEN运行pyspark" class="headerlink" title="Hadoop YAEN运行pyspark"></a>Hadoop YAEN运行pyspark</h4><p><strong>pyspark YARN运行</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/etc/hadoop pyspark --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure>
<h4 id="Spark-Standalone-Cluster运行"><a href="#Spark-Standalone-Cluster运行" class="headerlink" title="Spark Standalone Cluster运行"></a>Spark Standalone Cluster运行</h4><p><strong>复制模板文件创建spark-env.sh</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/<span class="built_in">local</span>/spark/conf/spark-env.sh.template /usr/<span class="built_in">local</span>/spark/conf/spark-env.sh</span><br></pre></td></tr></table></figure>
<p><strong>spark-env.sh</strong><br>分别设置master的名称、每个worker使用的CPU核心、使用内存、设置实例数<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=master</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_CORES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_MEMORY=512m</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_INSTANCES=4</span><br></pre></td></tr></table></figure></p>
<p><strong>将master的spark程序复制到data1</strong></p>
<p>首先ssh连接data1，连接成功后创建spark目录并更改所有者为hduser，使用scp命令。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp -r /usr/<span class="built_in">local</span>/spark hduser@data1:/usr/<span class="built_in">local</span></span><br></pre></td></tr></table></figure>
<p><strong>启动Spark Standalone Cluster</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p><strong>Spark Standalone运行pyspark</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master spark://master:7077 --num-executors 1 --total-executor-cores 3 --executor-memory 512m</span><br></pre></td></tr></table></figure>
<h4 id="Spark-Web-UI界面"><a href="#Spark-Web-UI界面" class="headerlink" title="Spark Web UI界面"></a>Spark Web UI界面</h4><p>在浏览器输入<a href="http://master:8080/" target="_blank" rel="noopener">http://master:8080/</a> 查看Spark Standalone Web UI界面。</p>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark-web.jpg" alt="Spark Web"></p>
<p>单击PySparkShell链接可以看Spark Jobs，单击其中的Job可以查看详细执行过程，用于调试或Performance Tuning（性能调优）时找出那些执行缓慢的Job。</p>
<p><strong>停止Spark Standalone Cluster</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/spark/sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<h4 id="Spark总结"><a href="#Spark总结" class="headerlink" title="Spark总结"></a>Spark总结</h4><p>pyspark可以在不同模式下执行、获取本地以及HDFS文件，但是pyspark是在“终端”程序中使用纯文本界面时不方便，且执行的命令与结果无法记录，我们可以采用IPython Notebook来将数据分析的过程、运行后的命令与结构存储成笔记本，下次再打开笔记本时可以重新执行这些命令。</p>
<h3 id="IPython-Notebook安装和运行"><a href="#IPython-Notebook安装和运行" class="headerlink" title="IPython Notebook安装和运行"></a>IPython Notebook安装和运行</h3><h4 id="安装Anaconda"><a href="#安装Anaconda" class="headerlink" title="安装Anaconda"></a>安装Anaconda</h4><p><strong>bash安装Anaconda</strong></p>
<p>安装的方式大同小异，不过Anaconda下载下来是.sh文件，安装时使用bash命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2019.03-Linux-x86_64.sh -b</span><br></pre></td></tr></table></figure>
<p>“-b”指的是batch，按批次安装，会自动省略阅读license条款，自动安装到/home/hduser/anaconda3路径</p>
<p>在从机data1上也同样方法安装</p>
<p><strong>查看Python版本</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/Anaconda3.jpg" alt="Python 3.7.3"></p>
<h4 id="在IPython-Notebook使用Spark"><a href="#在IPython-Notebook使用Spark" class="headerlink" title="在IPython Notebook使用Spark"></a>在IPython Notebook使用Spark</h4><p><strong>创建ipynotebook工作目录</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/pythonwork/ipynotebook </span><br><span class="line"><span class="built_in">cd</span> ~/pythonwork/ipynotebook</span><br></pre></td></tr></table></figure>
<p><strong>在IPython Notebook界面运行pyspark</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=<span class="string">"notebook"</span> pyspark</span><br></pre></td></tr></table></figure>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/IPython%20Notebook.jpg" alt="IPython Notebook界面"></p>
<p>新建一个Notebook后会打开新页面，默认名字为Untitled，可以点击后修改名称。</p>
<p>在IPython Notebook的Cell中输入程序代码，然后按Shift+Enter或者Ctrl+Enter运行代码，差异如下：</p>
<blockquote>
<ul>
<li>Shift+Enter 运行后光标会移动到下一个Cell</li>
<li>Ctrl+Enter  运行后光标仍在当前Cell</li>
</ul>
</blockquote>
<p><img src="https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/IPython%20Notebook1.jpg" alt="IPython Notebook运行查看运行方式代码"></p>
<p>在退出Notebook之前，需要及时保存内容。输入Ctrl+C在终端进行关闭，输入y结束。</p>
<h4 id="IPython-Notebook总结"><a href="#IPython-Notebook总结" class="headerlink" title="IPython Notebook总结"></a>IPython Notebook总结</h4><p>IPython Notebook同样也有几种运行方式，也可以在Hadoop YARN-client模式、Spark Stand Alone模式运行。</p>
<h2 id="Spark第一次填坑"><a href="#Spark第一次填坑" class="headerlink" title="Spark第一次填坑"></a>Spark第一次填坑</h2><h3 id="Q1-启动HDFS缺少用户定义"><a href="#Q1-启动HDFS缺少用户定义" class="headerlink" title="Q1 启动HDFS缺少用户定义"></a>Q1 启动HDFS缺少用户定义</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [localhost] </span><br><span class="line">ERROR: Attempting to launch hdfs namenode as root </span><br><span class="line">ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting launch.</span><br></pre></td></tr></table></figure>
<p>解决办法：</p>
<p>修改tart-dfs.sh、stop-dfs.sh，加上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root </span><br><span class="line">HADOOP_SECURE_DN_USER=hdfs </span><br><span class="line">HDFS_NAMENODE_USER=root </span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>
<h3 id="Q2-启动YARN缺少用户定义"><a href="#Q2-启动YARN缺少用户定义" class="headerlink" title="Q2 启动YARN缺少用户定义"></a>Q2 启动YARN缺少用户定义</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Starting resourcemanager </span><br><span class="line">ERROR: Attempting to launch yarn resourcemanager as root </span><br><span class="line">ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting launch. </span><br><span class="line">Starting nodemanagers </span><br><span class="line">ERROR: Attempting to launch yarn nodemanager as root </span><br><span class="line">ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting launch.</span><br></pre></td></tr></table></figure>
<p>解决办法：</p>
<p>修改start-yarn.sh、stop-yarn.sh，加上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
<h3 id="Q3-50070端口打不开"><a href="#Q3-50070端口打不开" class="headerlink" title="Q3 50070端口打不开"></a>Q3 50070端口打不开</h3><p>发现可能是namenode初始化默认端口失效，于是决定手动修改配置文件设置默认端口</p>
<p>解决办法：</p>
<p>修改hdfs-site.xml，添加：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>再重新hadoop namenode -format初始化一遍。</p>
<h3 id="Q4-SSH从机始终需要输入密码"><a href="#Q4-SSH从机始终需要输入密码" class="headerlink" title="Q4 SSH从机始终需要输入密码"></a>Q4 SSH从机始终需要输入密码</h3><p>在SSH data1的时候，每次都需要输入密码，查了许多资料，有点说还需要修改一个文件，的确修改后成功免密登陆。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>
<p>找到以下内容，并去掉前边的#，如果没有对应文字内容，直接加上去：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RSAAuthentication yes</span><br><span class="line">PubkeyAuthentication yes</span><br><span class="line">AuthorizedKeysFile  .ssh/authorized_keys</span><br><span class="line">PasswordAuthentication no</span><br></pre></td></tr></table></figure>
<p>再重启sshd服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart ssh</span><br></pre></td></tr></table></figure></p>
<h3 id="Q5-start-all之后master机器jps没有ResourceManager和NodeManager进程"><a href="#Q5-start-all之后master机器jps没有ResourceManager和NodeManager进程" class="headerlink" title="Q5 start-all之后master机器jps没有ResourceManager和NodeManager进程"></a>Q5 start-all之后master机器jps没有ResourceManager和NodeManager进程</h3><p>在网上试了很多种办法，最终确定是我当时的JDK版本过高，Hadoop3.2使用JDK8即可正常运行这两个进程。</p>
<h3 id="Q6-Mapreduce运行异常"><a href="#Q6-Mapreduce运行异常" class="headerlink" title="Q6 Mapreduce运行异常"></a>Q6 Mapreduce运行异常</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Container exited with a non-zero <span class="built_in">exit</span> code 1</span><br></pre></td></tr></table></figure>
<p>解决办法：</p>
<p>新版本的hadoop的yarn中需要配置上正面的内容，于是在yarn-site.xml添加正面内容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>data1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>保存完同时覆盖集群其他节点上的这个文件，最后重启yarn再启动mapreduce应用，成功运行。</p>
<h2 id="接下来计划"><a href="#接下来计划" class="headerlink" title="接下来计划"></a>接下来计划</h2><p><strong>继续学习《Python+Spark2.0+Hadoop机器学习与大数据实战》</strong></p>
<ol>
<li>关于RDD基本运算、eclipse+pyDev的安装和运行返利程序</li>
<li>RDD为基础的Spark MLlib机器学习，包括创建推荐引擎、决策树二元分类、逻辑回归二元分类、SVM二元分类、朴素贝叶斯二元分类、决策树多元分类、决策树回归分析等</li>
<li>dataframe为基础的Spark ML Pipeline机器学习，包括SQL、DataFrame、RDD数据统计与可视化、流程二元\多元分类、流程回归分析。</li>
</ol>
<p><strong>开始学习《Spark高级数据分析》</strong></p>
<p><strong>解决第一次学习之路遗留问题</strong></p>

        </div>
        <footer class="article-footer">
            



    <a data-url="http://wenshunjie.top/posts/programming/2019-04-04-Spark学习之路（一）.html" data-id="cju8g1ujj000iaor33kevg1q5" class="article-share-link"><i class="fa fa-share"></i>分享到</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
</article>
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMzEyMi85Njgx">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>关注我 :</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/DylanWhen/dylanwhen.github.io" target="_blank">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="weibo" href="https://weibo.com/1771030775/" target="_blank">
                        <i class="icon fa fa-weibo"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/posts/life/2018-06-12-毕业设计三两事.html" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">上一篇</strong>
        <p class="article-nav-title">毕业设计三两事</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/posts/programming/2019-04-04-Spark学习之路（一）.html" class="thumbnail">
    
    
        <span style="background-image:url(https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/spark1/spark-logo-trademark.png)" alt="Spark学习之路（一）" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/programming/">滚键盘</a></p>
                            <p class="item-title"><a href="/posts/programming/2019-04-04-Spark学习之路（一）.html" class="title">Spark学习之路（一）</a></p>
                            <p class="item-date"><time datetime="2019-04-04T02:45:49.000Z" itemprop="datePublished">2019-04-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/posts/life/2018-06-12-毕业设计三两事.html" class="thumbnail">
    
    
        <span style="background-image:url(https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/large.jpg)" alt="毕业设计三两事" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/life/">过日子</a></p>
                            <p class="item-title"><a href="/posts/life/2018-06-12-毕业设计三两事.html" class="title">毕业设计三两事</a></p>
                            <p class="item-date"><time datetime="2018-06-12T10:00:00.000Z" itemprop="datePublished">2018-06-12</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/posts/life/2018-01-16-我的第一篇影评随笔.html" class="thumbnail">
    
    
        <span style="background-image:url(https://dylanwen-1256745329.cos.ap-chengdu.myqcloud.com/pic_blog/wuwenxidong.jpg)" alt="第一篇观影随笔" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/life/">过日子</a></p>
                            <p class="item-title"><a href="/posts/life/2018-01-16-我的第一篇影评随笔.html" class="title">第一篇观影随笔</a></p>
                            <p class="item-date"><time datetime="2018-01-16T02:49:49.000Z" itemprop="datePublished">2018-01-16</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/posts/life/2018-01-09-我的第一篇博文.html" class="thumbnail">
    
    
        <span style="background-image:url(https://i.loli.net/2018/01/11/5a575bb944417.jpg)" alt="我的第一篇博文" class="thumbnail-image"></span>
    
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/life/">过日子</a></p>
                            <p class="item-title"><a href="/posts/life/2018-01-09-我的第一篇博文.html" class="title">我的第一篇博文</a></p>
                            <p class="item-date"><time datetime="2018-01-09T15:39:49.000Z" itemprop="datePublished">2018-01-09</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/programming/">滚键盘</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/life/">过日子</a><span class="category-list-count">3</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">标签</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习/">学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/影评-无问西东-勇敢者游戏2-凶镇谜案/">影评 无问西东 勇敢者游戏2 凶镇谜案</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/生活/">生活</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/学习/" style="font-size: 10px;">学习</a> <a href="/tags/影评-无问西东-勇敢者游戏2-凶镇谜案/" style="font-size: 10px;">影评 无问西东 勇敢者游戏2 凶镇谜案</a> <a href="/tags/生活/" style="font-size: 10px;">生活</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>
                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2019 Dylan Wen</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
                 <span id="busuanzi_container_site_pv" style="display: inline;">站长统计<span id="busuanzi_value_site_pv"></span>次</span>
            </div>
        </div>
    </div>
</footer>
<script src="//cdn1.lncld.net/static/js/2.5.0/av-min.js"></script>
<script>
    var APP_ID = 'IFjKPoyR4UU8LhN5brVHTX3p-gzGzoHsz';
    var APP_KEY = 'KnztICOY7miwg7QckjXB6E1u';
    AV.init({
        appId: APP_ID,
        appKey: APP_KEY
    });
    // 显示次数
    function showTime(Counter) {
        var query = new AV.Query("Counter");
        if($(".leancloud_visitors").length > 0){
            var url = $(".leancloud_visitors").attr('id').trim();
            // where field
            query.equalTo("words", url);
            // count
            query.count().then(function (number) {
                // There are number instances of MyClass where words equals url.
                $(document.getElementById(url)).text(number?  number : '--');
            }, function (error) {
                // error is an instance of AVError.
            });
        }
    }
    // 追加pv
    function addCount(Counter) {
        var url = $(".leancloud_visitors").length > 0 ? $(".leancloud_visitors").attr('id').trim() : 'icafebolger.com';
        var Counter = AV.Object.extend("Counter");
        var query = new Counter;
        query.save({
            words: url
        }).then(function (object) {
        })
    }
    $(function () {
        var Counter = AV.Object.extend("Counter");
        addCount(Counter);
        showTime(Counter);
    });
    </script>

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

        


    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>
